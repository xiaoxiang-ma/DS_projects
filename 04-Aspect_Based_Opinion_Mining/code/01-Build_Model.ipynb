{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Sentence Classification Model Building\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse & clearn labeled training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse('../data/Restaurants_Train.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this dataframe for multilabel classification\n",
    "# Must use scikitlearn's multilabel binarizer\n",
    "\n",
    "labeled_reviews = []\n",
    "for sentence in root.findall(\"sentence\"):\n",
    "    entry = {}\n",
    "    aterms = []\n",
    "    aspects = []\n",
    "    if sentence.find(\"aspectTerms\"):\n",
    "        for aterm in sentence.find(\"aspectTerms\").findall(\"aspectTerm\"):\n",
    "            aterms.append(aterm.get(\"term\"))\n",
    "    if sentence.find(\"aspectCategories\"):\n",
    "        for aspect in sentence.find(\"aspectCategories\").findall(\"aspectCategory\"):\n",
    "            aspects.append(aspect.get(\"category\"))\n",
    "    entry[\"text\"], entry[\"terms\"], entry[\"aspects\"]= sentence[0].text, aterms, aspects\n",
    "    labeled_reviews.append(entry)\n",
    "labeled_df = pd.DataFrame(labeled_reviews)\n",
    "print(\"there are\",len(labeled_reviews),\"reviews in this training set\")\n",
    "#    print(sentence.find(\"aspectCategories\").findall(\"aspectCategory\").get(\"category\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save annotated reviews\n",
    "labeled_df.to_pickle(\"annotated_reviews_df.pkl\")\n",
    "labeled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model with Naive Bayes\n",
    "1. replace pronouns with neural coref\n",
    "2. train the model with naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'neuralcoref'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-400cfea4d814>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mneuralcoref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'neuralcoref'"
     ]
    }
   ],
   "source": [
    "import neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import en_core_web_lg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralcoref import Coref\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your usual SpaCy model (one of SpaCy English models)\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# load NeuralCoref and add it to the pipe of SpaCy's model\n",
    "import neuralcoref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coref = neuralcoref.NeuralCoref(nlp.vocab)\n",
    "nlp.add_pipe(coref, name='neuralcoref')\n",
    "\n",
    "# You're done. You can now use NeuralCoref the same way you usually manipulate a SpaCy document and it's annotations.\n",
    "doc = nlp(u'My sister has a dog. She loves him.')\n",
    "\n",
    "doc._.has_coref\n",
    "doc._.coref_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralcoref import Coref\n",
    "import en_core_web_lg\n",
    "spacy = en_core_web_lg.load()\n",
    "coref = Coref(nlp=spacy)\n",
    "\n",
    "# Define function for replacing pronouns using neuralcoref\n",
    "def replace_pronouns(text):\n",
    "    coref.one_shot_coref(text)\n",
    "    return coref.get_resolved_utterances()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read annotated reviews df, which is the labeled dataset for training\n",
    "# This is located in the pickled files folder\n",
    "annotated_reviews_df = pd.read_pickle(\"../pickled_files/annotated_reviews_df.pkl\")\n",
    "annotated_reviews_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for text whose pronouns have been replaced\n",
    "annotated_reviews_df[\"text_pro\"] = annotated_reviews_df.text.map(lambda x: replace_pronouns(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment below to pickle the new df\n",
    "# annotated_reviews_df.to_pickle(\"annotated_reviews_df2.pkl\")\n",
    "\n",
    "# Read pickled file with replaced pronouns if it exists already\n",
    "annotated_reviews_df = pd.read_pickle(\"annotated_reviews_df2.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Convert the multi-labels into arrays\n",
    "mlb = MultiLabelBinarizer()\n",
    "y = mlb.fit_transform(annotated_reviews_df.aspects)\n",
    "X = annotated_reviews_df.text_pro\n",
    "\n",
    "# Split data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# save the the fitted binarizer labels\n",
    "# This is important: it contains the how the multi-label was binarized, so you need to\n",
    "# load this in the next folder in order to undo the transformation for the correct labels.\n",
    "filename = 'mlb.pkl'\n",
    "pickle.dump(mlb, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "import numpy as np\n",
    "\n",
    "# LabelPowerset allows for multi-label classification\n",
    "# Build a pipeline for multinomial naive bayes classification\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words = \"english\",ngram_range=(1, 1))),\n",
    "                     ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                     ('clf', LabelPowerset(MultinomialNB(alpha=1e-1))),])\n",
    "text_clf = text_clf.fit(X_train, y_train)\n",
    "predicted = text_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if SVM performs better\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer()),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', LabelPowerset(\n",
    "                             SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, max_iter=6, random_state=42)))])\n",
    "_ = text_clf_svm.fit(X_train, y_train)\n",
    "predicted_svm = text_clf_svm.predict(X_test)\n",
    "\n",
    "#Calculate accuracy\n",
    "np.mean(predicted_svm == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Train naive bayes on full dataset and save model\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words = \"english\",ngram_range=(1, 1))),\n",
    "                     ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "                     ('clf', LabelPowerset(MultinomialNB(alpha=1e-1))),])\n",
    "text_clf = text_clf.fit(X, y)\n",
    "\n",
    "# save the model to disk\n",
    "filename = 'naive_model1.pkl'\n",
    "pickle.dump(text_clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we can move on to 02-Sentiment analysis notebook, which will load the fitted Naive bayes model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mlb.inverse_transform(predicted)\n",
    "pred_df = pd.DataFrame(\n",
    "    {'text_pro': X_test,\n",
    "     'pred_category': mlb.inverse_transform(predicted)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some scrap code below which wasn't used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save annotated reviews\n",
    "labeled_df.to_pickle(\"annotated_reviews_df.pkl\")\n",
    "labeled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was for parsing out terms & their relations to aspects\n",
    "# However, the terms were not always hyponyms of the aspects, so they were unusable\n",
    "aspects = {\"food\":[],\"service\":[],\"anecdotes/miscellaneous\":[], \"ambience\":[], \"price\":[]}\n",
    "for i in range(len(labeled_df)):\n",
    "    if len(labeled_df.aspects[i]) == 1:\n",
    "        if labeled_df.terms[i] != []:\n",
    "            for terms in labeled_df.terms[i]:\n",
    "                aspects[labeled_df.aspects[i][0]].append(terms.lower())\n",
    "for key in aspects:\n",
    "    aspects[key] = list(set(aspects[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = []\n",
    "for i in labeled_df.terms:\n",
    "    for j in i:\n",
    "        if j not in terms:\n",
    "            terms.append(j)\n",
    "print(\"there are\", len(terms),\"unique terms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this dataframe if doing the classifications separately as binary classifications\n",
    "labeled_reviews2 = []\n",
    "for sentence in root.findall(\"sentence\"):\n",
    "    entry = {\"food\":0,\"service\":0,\"anecdotes/miscellaneous\":0, \"ambience\":0, \"price\":0}\n",
    "    aterms = []\n",
    "    aspects = []\n",
    "    if sentence.find(\"aspectTerms\"):\n",
    "        for aterm in sentence.find(\"aspectTerms\").findall(\"aspectTerm\"):\n",
    "            aterms.append(aterm.get(\"term\"))\n",
    "    if sentence.find(\"aspectCategories\"):\n",
    "        for aspect in sentence.find(\"aspectCategories\").findall(\"aspectCategory\"):\n",
    "            if aspect.get(\"category\") in entry.keys():\n",
    "                entry[aspect.get(\"category\")] = 1\n",
    "    entry[\"text\"], entry[\"terms\"] = sentence[0].text, aterms\n",
    "    labeled_reviews2.append(entry)\n",
    "labeled_df2 = pd.DataFrame(labeled_reviews2)\n",
    "#    print(sentence.find(\"aspectCategories\").findall(\"aspectCategory\").get(\"category\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_df2.iloc[:,:5].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
